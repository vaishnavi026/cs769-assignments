{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = [[[-0.1, -0.4, -0.5, -0.6], [-0.4, -0.8, -0.4, -0.3], [-0.9, -0.5, -0.2, -0.7]],\n",
    "                 [[-0.6, -0.3, -0.8, -0.9], [-0.9, -0.5, -0.7, -0.4], [-0.7, -0.8, -0.2, -0.8]]]\n",
    "top_score = [[-0.1, -0.2, -0.3], [-0.2, -0.3, -0.4]]\n",
    "top_beamid = [[0, 2, 1], [2, 0, 1]]\n",
    "top_wordid = [[0, 2, 3], [2, 1, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "score= torch.FloatTensor(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 4)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, beam_size, vocab_size = score.shape\n",
    "batch_size, beam_size, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, beam_size, vocab_size = score.shape\n",
    "top_score = torch.empty(batch_size,beam_size)\n",
    "top_beamid = torch.empty(batch_size,beam_size)\n",
    "top_wordid = torch.empty(batch_size,beam_size)\n",
    "for batch in range(batch_size):\n",
    "    score_batch = score[batch].view(-1)\n",
    "    topKscores, unparsed_indices = torch.topk(score_batch, beam_size)\n",
    "    word_indices = unparsed_indices % vocab_size\n",
    "    beam_indices = unparsed_indices // vocab_size\n",
    "    top_score[batch] = topKscores\n",
    "    top_beamid[batch] = beam_indices\n",
    "    top_wordid[batch] = word_indices\n",
    "    # print(top_score)\n",
    "    # print(topKscores, word_indices, beam_indices)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1000, -0.2000, -0.3000],\n",
       "         [-0.2000, -0.3000, -0.4000]]),\n",
       " tensor([[0., 2., 1.],\n",
       "         [2., 0., 1.]]),\n",
       " tensor([[0., 2., 3.],\n",
       "         [2., 1., 3.]]))"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_score, top_beamid, top_wordid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = torch.LongTensor(batch_size, beam_size, decode_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.1000, -0.2000, -0.3000]), tensor([ 0, 10,  7]))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = score.view(2, -1)\n",
    "values, indices = torch.topk(score[0], 3)\n",
    "values, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_scores = torch.FloatTensor(batch_size, beam_size)\n",
    "top_beamids = torch.LongTensor(batch_size, beam_size)\n",
    "top_scores = torch.LongTensor(batch_size, beam_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.max(score[0][0]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'long' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/u/v/a/vaishnavi_b/NLP/cs769-assignments/assignment4/help.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Binstgpu-04.wisc.edu/u/v/a/vaishnavi_b/NLP/cs769-assignments/assignment4/help.ipynb#X55sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m long(k)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'long' is not defined"
     ]
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_scores[0][0] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.10000000149011612\n",
      "-0.30000001192092896\n",
      "-0.20000000298023224\n",
      "-0.30000001192092896\n",
      "-0.4000000059604645\n",
      "-0.20000000298023224\n"
     ]
    }
   ],
   "source": [
    "for batch in range(batch_size):\n",
    "    for beam in range(beam_size):\n",
    "        print(torch.max(score[batch][beam]).item())\n",
    "\n",
    "        top_scores[batch][beam] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.10000000149011612"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't assign a builtin_function_or_method to a torch.LongTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/u/v/a/vaishnavi_b/NLP/cs769-assignments/assignment4/help.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Binstgpu-04.wisc.edu/u/v/a/vaishnavi_b/NLP/cs769-assignments/assignment4/help.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m top_scores[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39;49m] \u001b[39m=\u001b[39m k\u001b[39m.\u001b[39mvalues\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Binstgpu-04.wisc.edu/u/v/a/vaishnavi_b/NLP/cs769-assignments/assignment4/help.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m top_scores[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m] \n",
      "\u001b[0;31mTypeError\u001b[0m: can't assign a builtin_function_or_method to a torch.LongTensor"
     ]
    }
   ],
   "source": [
    "top_scores[0][0] = k.values\n",
    "top_scores[0][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_scores[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1000, -0.2000, -0.3000],\n",
       "         [-0.2000, -0.3000, -0.4000]]),\n",
       " tensor([[0, 2, 1],\n",
       "         [2, 0, 1]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_K_beams, top_beamid = torch.sort(top_canditate, dim = 1, descending=True)\n",
    "top_K_beams, top_beamid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topK(score):\n",
    "    \"\"\"\n",
    "    For every example in a batch, we have generated K candidates (partial sequences) in beam search (K=beam_size).\n",
    "    For each candidate, we will search for the next token from a vocabulary of V tokens (V=vocab_size). \n",
    "    So, we expand the old candidates to get (K * V) new candidates in total, and then we will prune the new\n",
    "    candidates to only keep the top K candidates in the beam.\n",
    "    Args: \n",
    "        score: torch.FloatTensor, [batch_size, beam_size, vocab_size]. \n",
    "               This tensor has the cummulated score of selecting the next token from a vocabuary\n",
    "               for each old candidate for each example from a batch.\n",
    "    Return:\n",
    "        top_score: torch.FloatTensor, [batch_size, beam_size], the scores of the top K new candidates in the beam after pruning \n",
    "        top_beamid: torch.LongTensor, [batch_size, beam_size], the beam ids of the top K new candidates\n",
    "        top_wordid: torch.LongTensor, [batch_size, beam_size], the word ids of the next tokens to construct the top K new candidates\n",
    "    \n",
    "    Example:\n",
    "        Assuming batch_size = 2, beam_size = 3, vocab_size = 4, we have the inputs and outputs as follows:\n",
    "        score = [[[-0.1, -0.4, -0.5, -0.6], [-0.4, -0.8, -0.4, -0.3], [-0.9, -0.5, -0.2, -0.7]],\n",
    "                 [[-0.6, -0.3, -0.8, -0.9], [-0.9, -0.5, -0.7, -0.4], [-0.7, -0.8, -0.2, -0.8]]]\n",
    "        top_score = [[-0.1, -0.2, -0.3], [-0.2, -0.3, -0.4]]\n",
    "        top_beamid = [[0, 2, 1], [2, 0, 1]]\n",
    "        top_wordid = [[0, 2, 3], [2, 1, 3]]  \n",
    "\n",
    "    \"\"\"\n",
    "    top_score, top_wordid = score.max(dim = 2)\n",
    "    top_score, top_beamid = torch.sort(top_score, dim = 1, descending=True)\n",
    "    top_wordid = torch.gather(top_wordid, dim=1, index=top_beamid)\n",
    "    return top_score, top_beamid, top_wordid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1000, -0.2000, -0.3000],\n",
       "         [-0.2000, -0.3000, -0.4000]]),\n",
       " tensor([[0, 2, 1],\n",
       "         [2, 0, 1]]),\n",
       " tensor([[0, 2, 3],\n",
       "         [2, 1, 3]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_score, top_beamid, top_wordid = topK(score)\n",
    "top_score, top_beamid, top_wordid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.1000, -0.4000, -0.5000, -0.6000],\n",
       "          [-0.3000, -0.4000, -0.4000, -0.8000],\n",
       "          [-0.2000, -0.5000, -0.7000, -0.9000]],\n",
       " \n",
       "         [[-0.3000, -0.6000, -0.8000, -0.9000],\n",
       "          [-0.4000, -0.5000, -0.7000, -0.9000],\n",
       "          [-0.2000, -0.7000, -0.8000, -0.8000]]]),\n",
       " tensor([[[0, 1, 2, 3],\n",
       "          [3, 0, 2, 1],\n",
       "          [2, 1, 3, 0]],\n",
       " \n",
       "         [[1, 0, 2, 3],\n",
       "          [3, 1, 2, 0],\n",
       "          [2, 0, 1, 3]]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted, indices = torch.sort(score, dim = 2, descending=True)\n",
    "sorted, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.1000, -0.4000, -0.5000, -0.6000],\n",
       "          [-0.3000, -0.4000, -0.4000, -0.8000],\n",
       "          [-0.2000, -0.5000, -0.7000, -0.9000]],\n",
       " \n",
       "         [[-0.3000, -0.6000, -0.8000, -0.9000],\n",
       "          [-0.4000, -0.5000, -0.7000, -0.9000],\n",
       "          [-0.2000, -0.7000, -0.8000, -0.8000]]]),\n",
       " tensor([[[0, 1, 2, 3],\n",
       "          [3, 0, 2, 1],\n",
       "          [2, 1, 3, 0]],\n",
       " \n",
       "         [[1, 0, 2, 3],\n",
       "          [3, 1, 2, 0],\n",
       "          [2, 0, 1, 3]]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_1, indices_1 = torch.sort(sorted, dim = 2, descending=True)\n",
    "sorted, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sorted = sorted[:, : , 0]\n",
    "t_indices = indices[:, : , 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable module object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/u/v/a/vaishnavi_b/NLP/cs769-assignments/assignment4/help.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Binstgpu-04.wisc.edu/u/v/a/vaishnavi_b/NLP/cs769-assignments/assignment4/help.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m t_sorted, t_indices \u001b[39m=\u001b[39m torch\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable module object"
     ]
    }
   ],
   "source": [
    "t_sorted, t_indices = torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/u/v/a/vaishnavi_b/NLP/cs769-assignments/assignment4/help.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Binstgpu-04.wisc.edu/u/v/a/vaishnavi_b/NLP/cs769-assignments/assignment4/help.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m sortedScores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49msort(score\u001b[39m.\u001b[39;49mmax(dim \u001b[39m=\u001b[39;49m \u001b[39m2\u001b[39;49m)\u001b[39m.\u001b[39;49mvalues, dim\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, descending \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "sortedScores = torch.sort(score.max(dim = 2).values, dim=2, descending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sortedScores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/u/v/a/vaishnavi_b/NLP/cs769-assignments/assignment4/help.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Binstgpu-04.wisc.edu/u/v/a/vaishnavi_b/NLP/cs769-assignments/assignment4/help.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m sortedScores\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sortedScores' is not defined"
     ]
    }
   ],
   "source": [
    "sortedScores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddens = torch.FloatTensor([[[0.6566, 0.2719, 0.7182, 0.1083],\n",
    "                                    [0.1627, 0.4812, 0.1167, 0.4318],\n",
    "                                    [0.1817, 0.2578, 0.5769, 0.2610]],\n",
    "\n",
    "                                    [[0.9372, 0.4993, 0.5471, 0.9169],\n",
    "                                    [0.8798, 0.6168, 0.6097, 0.8790],\n",
    "                                    [0.6642, 0.4301, 0.5542, 0.3670]]])\n",
    "beam_id = torch.LongTensor([[[1,1,1,1], [1,1,1,1], [0,0,0,0]], [[2,2,2,2], [0,0,0,0],[0,0,0,0]]])\n",
    "# new_hiddens = torch.FloatTensor([[[0.1627, 0.4812, 0.1167, 0.4318],\n",
    "#                                 [0.1627, 0.4812, 0.1167, 0.4318],\n",
    "#                                 [0.6566, 0.2719, 0.7182, 0.1083]],\n",
    "\n",
    "#                                 [[0.6642, 0.4301, 0.5542, 0.3670],\n",
    "#                                 [0.9372, 0.4993, 0.5471, 0.9169],\n",
    "#                                         [0.9372, 0.4993, 0.5471, 0.9169]]])\n",
    "    # \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1, 1],\n",
       "         [1, 1, 1, 1],\n",
       "         [0, 0, 0, 0]],\n",
       "\n",
       "        [[2, 2, 2, 2],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_beam_id\n",
    "expanded_beam_id = beam_id.unsqueeze(2).expand(-1, -1, hiddens.size(2))\n",
    "expanded_beam_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1627, 0.4812, 0.1167, 0.4318],\n",
       "         [0.1627, 0.4812, 0.1167, 0.4318],\n",
       "         [0.6566, 0.2719, 0.7182, 0.1083]],\n",
       "\n",
       "        [[0.6642, 0.4301, 0.5542, 0.3670],\n",
       "         [0.9372, 0.4993, 0.5471, 0.9169],\n",
       "         [0.9372, 0.4993, 0.5471, 0.9169]]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_hiddens = torch.gather(hiddens, 1, expanded_beam_id)\n",
    "new_hiddens = torch.gather(hiddens, 1, expanded_beam_id)\n",
    "new_hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 0],\n",
       "        [2, 0, 0]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_id= torch.LongTensor([[1, 1, 0], [2, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz, beam_sz = beam_id.shape\n",
    "new_hiddens = torch.zeros_like(hiddens)\n",
    "for i in range(batch_sz):\n",
    "    for j in range(beam_sz):\n",
    "        new_hiddens[i][j] = hiddens[i][beam_id[i][j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1627, 0.4812, 0.1167, 0.4318],\n",
       "         [0.1627, 0.4812, 0.1167, 0.4318],\n",
       "         [0.6566, 0.2719, 0.7182, 0.1083]],\n",
       "\n",
       "        [[0.6642, 0.4301, 0.5542, 0.3670],\n",
       "         [0.9372, 0.4993, 0.5471, 0.9169],\n",
       "         [0.9372, 0.4993, 0.5471, 0.9169]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_score = torch.Tensor([[ 0.00, -0.02, -0.03, -0.03, -0.04],\n",
    "                              [ 0.00, -0.00, -0.03, -0.03, -0.03],\n",
    "                              [ 0.00, -0.00, -0.11, -0.11, -0.13]])\n",
    "top_wordids = [\n",
    "    torch.LongTensor([[ 3, 12,  5,  4,  9], [12, 11,  9,  5,  8], [ 3,  8, 11,  4,  5]]), \n",
    "    torch.LongTensor([[12,  1,  0, 12, 12], [ 7,  7,  7,  4,  7], [ 4,  0,  2,  7,  4]]), \n",
    "    torch.LongTensor([[13,  2, 13, 13, 13], [ 8,  5,  8,  5,  8], [ 9,  9,  2,  2,  9]]), \n",
    "    torch.LongTensor([[ 8,  2,  8,  8,  8], [ 5,  5,  5,  5,  5], [11, 11,  1,  1,  9]]), \n",
    "    torch.LongTensor([[ 5,  2,  5,  5,  5], [10, 10, 10, 10, 10], [ 6,  6,  2,  2,  0]]), \n",
    "    torch.LongTensor([[6, 7, 2, 6, 6], [6, 6, 7, 6, 7], [1, 1, 2, 2, 1]]), \n",
    "    torch.LongTensor([[1, 1, 2, 1, 1], [1, 1, 1, 1, 1], [2, 2, 2, 2, 2]])\n",
    "]\n",
    "top_beamids = [\n",
    "    torch.LongTensor([[1, 4, 1, 1, 2], [0, 1, 0, 0, 0], [2, 3, 4, 4, 2]]), \n",
    "    torch.LongTensor([[0, 1, 0, 2, 3], [0, 2, 3, 0, 4], [0, 0, 1, 2, 4]]), \n",
    "    torch.LongTensor([[0, 1, 2, 3, 4], [0, 0, 1, 1, 2], [0, 1, 0, 1, 4]]), \n",
    "    torch.LongTensor([[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 0, 1, 0]]), \n",
    "    torch.LongTensor([[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 0]]),\n",
    "    torch.LongTensor([[0, 0, 1, 2, 3], [0, 1, 0, 2, 1], [0, 1, 2, 3, 4]]), \n",
    "    torch.LongTensor([[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4]])\n",
    "]\n",
    "sequence = torch.LongTensor([[[ 3, 12, 13,  8,  5,  6,  1],\n",
    "                            [ 3, 12, 13,  8,  5,  7,  1],\n",
    "                            [12,  1,  2,  2,  2,  2,  2],\n",
    "                            [ 3,  0, 13,  8,  5,  6,  1],\n",
    "                            [ 5, 12, 13,  8,  5,  6,  1]],\n",
    "\n",
    "                            [[12,  7,  8,  5, 10,  6,  1],\n",
    "                            [12,  7,  5,  5, 10,  6,  1],\n",
    "                            [12,  7,  8,  5, 10,  7,  1],\n",
    "                            [ 9,  7,  8,  5, 10,  6,  1],\n",
    "                            [12,  7,  5,  5, 10,  7,  1]],\n",
    "\n",
    "                            [[ 3,  4,  9, 11,  6,  1,  2],\n",
    "                            [ 3,  0,  9, 11,  6,  1,  2],\n",
    "                            [ 3,  4,  9,  1,  2,  2,  2],\n",
    "                            [ 3,  0,  9,  1,  2,  2,  2],\n",
    "                            [ 3,  4,  9, 11,  0,  1,  2]]])\n",
    "sort_score = torch.FloatTensor([[ 0.00, -0.02, -0.03, -0.03, -0.04],\n",
    "                                [ 0.00, -0.00, -0.03, -0.03, -0.03],\n",
    "                                [ 0.00, -0.00, -0.11, -0.11, -0.13]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_beamids[6][0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_beamids[6][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 5 7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size, beam_size =  top_score.shape\n",
    "decode_max_length = len(top_wordids)\n",
    "sequences = torch.LongTensor(batch_size, beam_size, decode_max_length)\n",
    "print(batch_size, beam_size, decode_max_length)\n",
    "batch =[]\n",
    "for i in range(batch_size):\n",
    "    beam = []\n",
    "    for j in range(beam_size):\n",
    "        seq = []\n",
    "        beam_selected = j\n",
    "        for k in range(decode_max_length-1, -1, -1):\n",
    "            # print(i, j, k)\n",
    "            sequences[i][j][k] = top_wordids[k][i][beam_selected]\n",
    "            beam_selected = top_beamids[k][i][beam_selected]\n",
    "                 \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3, 12, 13,  8,  5,  6,  1],\n",
       "         [ 3, 12, 13,  8,  5,  7,  1],\n",
       "         [12,  1,  2,  2,  2,  2,  2],\n",
       "         [ 3,  0, 13,  8,  5,  6,  1],\n",
       "         [ 5, 12, 13,  8,  5,  6,  1]],\n",
       "\n",
       "        [[12,  7,  8,  5, 10,  6,  1],\n",
       "         [12,  7,  5,  5, 10,  6,  1],\n",
       "         [12,  7,  8,  5, 10,  7,  1],\n",
       "         [ 9,  7,  8,  5, 10,  6,  1],\n",
       "         [12,  7,  5,  5, 10,  7,  1]],\n",
       "\n",
       "        [[ 3,  4,  9, 11,  6,  1,  2],\n",
       "         [ 3,  0,  9, 11,  6,  1,  2],\n",
       "         [ 3,  4,  9,  1,  2,  2,  2],\n",
       "         [ 3,  0,  9,  1,  2,  2,  2],\n",
       "         [ 3,  4,  9, 11,  0,  1,  2]]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/u/v/a/vaishnavi_b/NLP/cs769-assignments/assignment4/help.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Binstgpu-04.wisc.edu/u/v/a/vaishnavi_b/NLP/cs769-assignments/assignment4/help.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m seq[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39m][\u001b[39m6\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "sequence = torch.LongTensor([[[ 3, 12, 13,  8,  5,  6,  1],\n",
    "                            [ 3, 12, 13,  8,  5,  7,  1],\n",
    "                            [12,  1,  2,  2,  2,  2,  2],\n",
    "                            [ 3,  0, 13,  8,  5,  6,  1],\n",
    "                            [ 5, 12, 13,  8,  5,  6,  1]],\n",
    "\n",
    "                            [[12,  7,  8,  5, 10,  6,  1],\n",
    "                            [12,  7,  5,  5, 10,  6,  1],\n",
    "                            [12,  7,  8,  5, 10,  7,  1],\n",
    "                            [ 9,  7,  8,  5, 10,  6,  1],\n",
    "                            [12,  7,  5,  5, 10,  7,  1]],\n",
    "\n",
    "                            [[ 3,  4,  9, 11,  6,  1,  2],\n",
    "                            [ 3,  0,  9, 11,  6,  1,  2],\n",
    "                            [ 3,  4,  9,  1,  2,  2,  2],\n",
    "                            [ 3,  0,  9,  1,  2,  2,  2],\n",
    "                            [ 3,  4,  9, 11,  0,  1,  2]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 7])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = torch.LongTensor(batch_size, beam_size, decode_max_length)\n",
    "sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.LongTensor([[1, 4, 1, 1, 2], [0, 1, 0, 0, 0], [2, 3, 4, 4, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

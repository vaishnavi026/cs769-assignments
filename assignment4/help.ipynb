{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = [[[-0.1, -0.4, -0.5, -0.6], [-0.4, -0.8, -0.4, -0.3], [-0.9, -0.5, -0.2, -0.7]],\n",
    "                 [[-0.6, -0.3, -0.8, -0.9], [-0.9, -0.5, -0.7, -0.4], [-0.7, -0.8, -0.2, -0.8]]]\n",
    "top_score = [[-0.1, -0.2, -0.3], [-0.2, -0.3, -0.4]]\n",
    "top_beamid = [[0, 2, 1], [2, 0, 1]]\n",
    "top_wordid = [[0, 2, 3], [2, 1, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = torch.tensor(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1000, -0.3000, -0.2000],\n",
       "         [-0.3000, -0.4000, -0.2000]]),\n",
       " tensor([[0, 3, 2],\n",
       "         [1, 3, 2]]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_canditate, wordId = score.max(dim = 2)\n",
    "top_canditate, wordId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1000, -0.2000, -0.3000],\n",
       "         [-0.2000, -0.3000, -0.4000]]),\n",
       " tensor([[0, 2, 1],\n",
       "         [2, 0, 1]]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_K_beams, top_beamid = torch.sort(top_canditate, dim = 1, descending=True)\n",
    "top_K_beams, top_beamid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topK(score):\n",
    "    \"\"\"\n",
    "    For every example in a batch, we have generated K candidates (partial sequences) in beam search (K=beam_size).\n",
    "    For each candidate, we will search for the next token from a vocabulary of V tokens (V=vocab_size). \n",
    "    So, we expand the old candidates to get (K * V) new candidates in total, and then we will prune the new\n",
    "    candidates to only keep the top K candidates in the beam.\n",
    "    Args: \n",
    "        score: torch.FloatTensor, [batch_size, beam_size, vocab_size]. \n",
    "               This tensor has the cummulated score of selecting the next token from a vocabuary\n",
    "               for each old candidate for each example from a batch.\n",
    "    Return:\n",
    "        top_score: torch.FloatTensor, [batch_size, beam_size], the scores of the top K new candidates in the beam after pruning \n",
    "        top_beamid: torch.LongTensor, [batch_size, beam_size], the beam ids of the top K new candidates\n",
    "        top_wordid: torch.LongTensor, [batch_size, beam_size], the word ids of the next tokens to construct the top K new candidates\n",
    "    \n",
    "    Example:\n",
    "        Assuming batch_size = 2, beam_size = 3, vocab_size = 4, we have the inputs and outputs as follows:\n",
    "        score = [[[-0.1, -0.4, -0.5, -0.6], [-0.4, -0.8, -0.4, -0.3], [-0.9, -0.5, -0.2, -0.7]],\n",
    "                 [[-0.6, -0.3, -0.8, -0.9], [-0.9, -0.5, -0.7, -0.4], [-0.7, -0.8, -0.2, -0.8]]]\n",
    "        top_score = [[-0.1, -0.2, -0.3], [-0.2, -0.3, -0.4]]\n",
    "        top_beamid = [[0, 2, 1], [2, 0, 1]]\n",
    "        top_wordid = [[0, 2, 3], [2, 1, 3]]  \n",
    "\n",
    "    \"\"\"\n",
    "    top_score, top_wordid = score.max(dim = 2)\n",
    "    top_score, top_beamid = torch.sort(top_score, dim = 1, descending=True)\n",
    "    top_wordid = torch.gather(top_wordid, dim=1, index=top_beamid)\n",
    "    return top_score, top_beamid, top_wordid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1000, -0.2000, -0.3000],\n",
       "         [-0.2000, -0.3000, -0.4000]]),\n",
       " tensor([[0, 2, 1],\n",
       "         [2, 0, 1]]),\n",
       " tensor([[0, 2, 3],\n",
       "         [2, 1, 3]]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_score, top_beamid, top_wordid = topK(score)\n",
    "top_score, top_beamid, top_wordid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.1000, -0.4000, -0.5000, -0.6000],\n",
       "          [-0.3000, -0.4000, -0.4000, -0.8000],\n",
       "          [-0.2000, -0.5000, -0.7000, -0.9000]],\n",
       " \n",
       "         [[-0.3000, -0.6000, -0.8000, -0.9000],\n",
       "          [-0.4000, -0.5000, -0.7000, -0.9000],\n",
       "          [-0.2000, -0.7000, -0.8000, -0.8000]]]),\n",
       " tensor([[[0, 1, 2, 3],\n",
       "          [3, 0, 2, 1],\n",
       "          [2, 1, 3, 0]],\n",
       " \n",
       "         [[1, 0, 2, 3],\n",
       "          [3, 1, 2, 0],\n",
       "          [2, 0, 1, 3]]]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted, indices = torch.sort(score, dim = 2, descending=True)\n",
    "sorted, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_1, indices_1 = torch.sort(sorted, dim = 2, descending=True)\n",
    "sorted, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sorted = sorted[:, : , 0]\n",
    "t_indices = indices[:, : , 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1000, -0.3000, -0.2000],\n",
       "         [-0.3000, -0.4000, -0.2000]]),\n",
       " tensor([[0, 3, 2],\n",
       "         [1, 3, 2]]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_sorted, t_indices = torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/u/v/a/vaishnavi_b/NLP/cs769-assignments/assignment4/help.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Binstgpu-04.wisc.edu/u/v/a/vaishnavi_b/NLP/cs769-assignments/assignment4/help.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m sortedScores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49msort(score\u001b[39m.\u001b[39;49mmax(dim \u001b[39m=\u001b[39;49m \u001b[39m2\u001b[39;49m)\u001b[39m.\u001b[39;49mvalues, dim\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, descending \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "sortedScores = torch.sort(score.max(dim = 2).values, dim=2, descending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([[[-0.1000, -0.4000, -0.5000, -0.6000],\n",
       "         [-0.3000, -0.4000, -0.4000, -0.8000],\n",
       "         [-0.2000, -0.5000, -0.7000, -0.9000]],\n",
       "\n",
       "        [[-0.3000, -0.6000, -0.8000, -0.9000],\n",
       "         [-0.4000, -0.5000, -0.7000, -0.9000],\n",
       "         [-0.2000, -0.7000, -0.8000, -0.8000]]]),\n",
       "indices=tensor([[[0, 1, 2, 3],\n",
       "         [3, 0, 2, 1],\n",
       "         [2, 1, 3, 0]],\n",
       "\n",
       "        [[1, 0, 2, 3],\n",
       "         [3, 1, 2, 0],\n",
       "         [2, 0, 1, 3]]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortedScores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddens = torch.FloatTensor([[[0.6566, 0.2719, 0.7182, 0.1083],\n",
    "                                    [0.1627, 0.4812, 0.1167, 0.4318],\n",
    "                                    [0.1817, 0.2578, 0.5769, 0.2610]],\n",
    "\n",
    "                                    [[0.9372, 0.4993, 0.5471, 0.9169],\n",
    "                                    [0.8798, 0.6168, 0.6097, 0.8790],\n",
    "                                    [0.6642, 0.4301, 0.5542, 0.3670]]])\n",
    "beam_id = torch.LongTensor([[[1,1,1,1], [1,1,1,1], [0,0,0,0]], [[2,2,2,2], [0,0,0,0],[0,0,0,0]]])\n",
    "# new_hiddens = torch.FloatTensor([[[0.1627, 0.4812, 0.1167, 0.4318],\n",
    "#                                 [0.1627, 0.4812, 0.1167, 0.4318],\n",
    "#                                 [0.6566, 0.2719, 0.7182, 0.1083]],\n",
    "\n",
    "#                                 [[0.6642, 0.4301, 0.5542, 0.3670],\n",
    "#                                 [0.9372, 0.4993, 0.5471, 0.9169],\n",
    "#                                         [0.9372, 0.4993, 0.5471, 0.9169]]])\n",
    "    # \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1, 1],\n",
       "         [1, 1, 1, 1],\n",
       "         [0, 0, 0, 0]],\n",
       "\n",
       "        [[2, 2, 2, 2],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_beam_idbeam_id= torch.LongTensor([[1, 1, 0], [2, 0, 0]])\n",
    "expanded_beam_id = beam_id.unsqueeze(2).expand(-1, -1, hiddens.size(2))\n",
    "expanded_beam_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1627, 0.4812, 0.1167, 0.4318],\n",
       "         [0.1627, 0.4812, 0.1167, 0.4318],\n",
       "         [0.6566, 0.2719, 0.7182, 0.1083]],\n",
       "\n",
       "        [[0.6642, 0.4301, 0.5542, 0.3670],\n",
       "         [0.9372, 0.4993, 0.5471, 0.9169],\n",
       "         [0.9372, 0.4993, 0.5471, 0.9169]]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_hiddens = torch.gather(hiddens, 1, expanded_beam_id)\n",
    "new_hiddens = torch.gather(hiddens, 1, expanded_beam_id)\n",
    "new_hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 0],\n",
       "        [2, 0, 0]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1627, 0.4812, 0.7182],\n",
       "         [0.1817, 0.2719, 0.7182]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
